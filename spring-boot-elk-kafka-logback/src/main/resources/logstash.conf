# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

input {
	kafka {
        bootstrap_servers => ["http://localhost:9092"]
		codec => "json"
		topics => "appkafkalog"   
		type => "kafka"
		group_id => "es"
		client_id => "kafka_client"
        consumer_threads => 1
        decorate_events => true
    }
	kafka {
        bootstrap_servers => ["http://localhost:9092"]
		codec => "json"
		topics => "goods"
		type => "kafka-goods"
		group_id => "es"
		client_id => "kafka_good_client"
        consumer_threads => 1
        decorate_events => true
    }
	# 安装logstash-input-jdbc插件 E:\elk\logstash-7.0.0>bin\logstash-plugin.bat install "E:/elk/logstash-input-jdbc-4.3.13.zip"
	jdbc {
		type => "jdbc-goods"
		jdbc_driver_library => "E:/apache-maven-3.5.3/repository/mysql/mysql-connector-java/8.0.11/mysql-connector-java-8.0.11.jar"
		jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
		jdbc_connection_string => "jdbc:mysql://localhost:3306/wise_base_goods?serverTimezone=Asia/Shanghai"
		jdbc_user => "root"
		jdbc_password => "root"
		#parameters => { "delFlag" => 0 } #参数
		jdbc_paging_enabled => "true" #是否分页
		jdbc_page_size => "100"	#分页大小
		schedule => "* * * * *"	#定时执行
		use_column_value => true
        record_last_run => true #记录上一次运行记录
		tracking_column => "ctimes"  #追踪字段名
		#tracking_column_type => "timestamp"  #字段类型
		# gmt_modified设置为根据当前时间戳跟新，当更新一条数据后，该字段也会更新，根据该字段是否更新更新es数据 ctimes是数据库中没有的字段该字段为时间戳类型，和gmt_modified进行比较
		# 当gmt_modified转换后的unix时间大于之前的时间:sql_last_value就会向es更新数据,新增数据时设置gmt_modified为当前时间那么新增的数据也会同步到es中
		# 插入或者更新后那么:sql_last_value(ctimes)变更为定时任务执行的时间
		# 定时执行后如果没有数据插入或者更新那么:sql_last_value(ctimes)保持不变
		statement => "select id,merchant_id,name,sku,img_url,category_name,gmt_create,gmt_modified,UNIX_TIMESTAMP(NOW()) as ctimes from goods where del_flag=0 and UNIX_TIMESTAMP(gmt_modified) >:sql_last_value"
		# 新增一条记录只要满足del_flag=0就会同步到es
		#statement => "select id,merchant_id,name,sku,img_url,category_name,gmt_create,gmt_modified from goods where del_flag=:delFlag"
    }
}

output {
#  stdout {  codec => rubydebug }
  if[type] == "kafka"{
		elasticsearch {
			hosts => ["http://localhost:9200"]
			index => "kafka-log-%{+YYYY.MM.dd}"
			#action => "kafka"
			#user => "elastic"
			#password => "changeme"
		}
  } else if[type] == "kafka-goods" {
		elasticsearch {
			hosts => ["http://localhost:9200"]
			index => "kafka-goods-%{+YYYY.MM.dd}"
			#action => "kafka"
			#user => "elastic"
			#password => "changeme"
		}
  } else if[type] == "jdbc-goods" {
		elasticsearch {
			hosts => ["http://localhost:9200"]
			index => "jdbc-goods"
			document_id => "%{id}" #id必须是待查询的数据表的序列字段
			document_type => "jdbc-goods" #type名称
			#action => "kafka"
			#user => "elastic"
			#password => "changeme"
		}
  }
  stdout {}
}
