# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

input {
	kafka {
        bootstrap_servers => ["http://localhost:9092"]
		codec => "json"
		topics => "appkafkalog"   
		type => "kafka"
		group_id => "es"
		client_id => "kafka_client"
        consumer_threads => 1
        decorate_events => true
    }
	kafka {
        bootstrap_servers => ["http://localhost:9092"]
		codec => "json"
		topics => "goods"
		type => "kafka-goods"
		group_id => "es"
		client_id => "kafka_good_client"
        consumer_threads => 1
        decorate_events => true
    }
	# 安装logstash-input-jdbc插件 E:\elk\logstash-7.0.0>bin\logstash-plugin.bat install "E:/elk/logstash-input-jdbc-4.3.13.zip"
	jdbc {
		type => "jdbc-goods"
		jdbc_driver_library => "E:/apache-maven-3.5.3/repository/mysql/mysql-connector-java/8.0.11/mysql-connector-java-8.0.11.jar"
		jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
		jdbc_connection_string => "jdbc:mysql://localhost:3306/wise_base_goods?serverTimezone=Asia/Shanghai"
		jdbc_user => "root"
		jdbc_password => "root"
		parameters => { "delFlag" => 0 } #参数
		jdbc_paging_enabled => "true" #是否分页
		jdbc_page_size => "100"	#分页大小
		schedule => "* * * * *"	#定时执行
		use_column_value => true
        record_last_run => true #记录上一次运行记录
		#tracking_column => "gmt_modified"  #追踪字段名
		#tracking_column_type => "timestamp"  #字段类型
		# gmt_modified设置为根据当前时间戳跟新，name当更新一条数据后，该字段也会更新，根据该字段是否更新更新es数据
		#statement => "select id,merchant_id,name,sku,img_url,category_name,gmt_create,gmt_modified from goods where del_flag=:delFlag and gmt_modified >:sql_last_value"
		statement => "select id,merchant_id,name,sku,img_url,category_name,gmt_create,gmt_modified from goods where del_flag=:delFlag "
    }
}

output {
#  stdout {  codec => rubydebug }
  if[type] == "kafka"{
		elasticsearch {
			hosts => ["http://localhost:9200"]
			index => "kafka-log-%{+YYYY.MM.dd}"
			#action => "kafka"
			#user => "elastic"
			#password => "changeme"
		}
  } else if[type] == "kafka-goods" {
		elasticsearch {
			hosts => ["http://localhost:9200"]
			index => "kafka-goods-%{+YYYY.MM.dd}"
			#action => "kafka"
			#user => "elastic"
			#password => "changeme"
		}
  } else if[type] == "jdbc-goods" {
		elasticsearch {
			hosts => ["http://localhost:9200"]
			index => "jdbc-goods"
			document_id => "%{id}" #id必须是待查询的数据表的序列字段
			document_type => "jdbc-goods" #type名称
			#action => "kafka"
			#user => "elastic"
			#password => "changeme"
		}
  }
  stdout {}
}
